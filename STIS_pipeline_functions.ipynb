{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial import chebyshev\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import medfilt, medfilt2d\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from datetime import datetime\n",
    "from matplotlib.dates import date2num, DateFormatter\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Libraries for plotting, reading data:\n",
    "import seaborn as sns \n",
    "sns.set_style(\"ticks\") # Set seaborn \"ticks\" style for better styled plots\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "# Library for some power-spectral density analysis:\n",
    "from astropy.timeseries import LombScargle\n",
    "\n",
    "# Corner (for posterior distribution plotting):\n",
    "import corner\n",
    "\n",
    "# Juliet (for transit fitting & model evaluation:)\n",
    "import juliet\n",
    "#plt.style.use('dark_background')\n",
    "\n",
    "from barycorrpy import utc_tdb\n",
    "from astropy.time import Time\n",
    "from transitspectroscopy import spectroscopy\n",
    "\n",
    "import batman\n",
    "import lmfit\n",
    "import dynesty\n",
    "from dynesty import plotting as dyplot\n",
    "from dynesty import utils as dyfunc\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import scipy\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from astropy.utils.data import get_pkg_data_filename, download_file\n",
    "from astropy.table import Table, Column, MaskedColumn\n",
    "from astropy.io import fits, ascii\n",
    "from astropy.modeling.models import custom_model\n",
    "from astropy.modeling.fitting import LevMarLSQFitter\n",
    "import astropy.units as u\n",
    "from scipy.interpolate import interp1d, splev, splrep\n",
    "import scipy.optimize as opt\n",
    "from scipy.io import readsav\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "import scipy.signal as signal\n",
    "import glob\n",
    "import lmfit\n",
    "import pickle\n",
    "from os import path,mkdir\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#import statsmodels.api as sm\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numba\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that opens each orbit fits file and gets the science exposures\n",
    "# optional kwargs to also get the dq extensions and jit vectors, for later use in cleaning/detrending the data\n",
    "# ADD: automatically throws out the first exposure of each orbit, option to also throw out the first orbit\n",
    "# ADD: error message if jit file doesn't exist\n",
    "def get_data(files, dq = True, jit = True, keep_first_orbit = True):\n",
    "    \n",
    "    # initializing some lists to hold the data\n",
    "    data_hold = []\n",
    "    error_hold = []\n",
    "    headers_hold = []\n",
    "    jitter_hold = []\n",
    "    dqs_hold = []\n",
    "    \n",
    "    # keeping track of how many exposures in each orbit\n",
    "    visits_idx = []\n",
    "    \n",
    "    \n",
    "    for fits_file in files:\n",
    "        hdulist = fits.open(fits_file)\n",
    "        sciextlist = []\n",
    "        errextlist = []\n",
    "        dqextlist = []\n",
    "\n",
    "        # finding which fits extensions correspond to the science, error, and dq frames, if applicable\n",
    "        for i in range(len(hdulist.info(0))):\n",
    "            if hdulist.info(0)[i][1] == \"SCI\":\n",
    "                sciextlist.append(i)\n",
    "            \n",
    "            if hdulist.info(0)[i][1] == \"ERR\":\n",
    "                errextlist.append(i)\n",
    "                \n",
    "            if dq == True:\n",
    "                if hdulist.info(0)[i][1] == \"DQ\":\n",
    "                    dqextlist.append(i)\n",
    "        \n",
    "        # getting the data and header for each of the science frames\n",
    "        data_lst = []\n",
    "        header_lst = []\n",
    "        for j in sciextlist:\n",
    "            data, header = fits.getdata(fits_file, ext = j, header=True)\n",
    "            # getting rid of one second exposures\n",
    "            if header[\"EXPTIME\"] !=  1.0:\n",
    "                data_lst.append(data)\n",
    "                header_lst.append(header)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        # getting the error frames\n",
    "        err_lst = []\n",
    "        for e in errextlist:\n",
    "            err = fits.getdata(fits_file, ext = e, header = False)\n",
    "            err_lst.append(err)\n",
    "\n",
    "        # getting the data quality frames\n",
    "        if dq == True:\n",
    "            dq_lst = []\n",
    "            for k in dqextlist:\n",
    "                dqs = fits.getdata(fits_file, ext = k, header = False)\n",
    "                dq_lst.append(dqs)\n",
    "                \n",
    "        if jit == True:\n",
    "            # gets corresponding .jit file for each .fits file\n",
    "            corresponding_jit_file = fits.open(fits_file.replace(\"flt\",\"jit\")) \n",
    "            \n",
    "            # gets the names of the different jitter vectors\n",
    "            jitter_vector_list = corresponding_jit_file[1].columns.names \n",
    "            \n",
    "            # initialize an intermediate list\n",
    "            jitter_lst = []\n",
    "\n",
    "            for jitter_hdu in corresponding_jit_file: # iterates through each exposure of each file\n",
    "                if jitter_hdu.name == 'jit':\n",
    "                    dummy_jit_array = []\n",
    "                    for jitvect in jitter_vector_list: # iterates through each jitter vector name\n",
    "                        jitter_points = jitter_hdu.data[jitvect]\n",
    "                        jitter_points[jitter_points > 1e30] = np.median(jitter_points) # kills weird edge cases\n",
    "                        dummy_jit_array.append(np.mean(jitter_points)) # saves the mean jitter value inside of each exposure\n",
    "                    jitter_lst.append(dummy_jit_array)\n",
    "        \n",
    "        \n",
    "        #i'm not returning this at the moment but can if they're needed\n",
    "        visits_idx.append(len(data_lst)) \n",
    "        \n",
    "        # adding each orbit's data to master list\n",
    "        data_hold = data_hold + data_lst\n",
    "        headers_hold = headers_hold + header_lst\n",
    "        error_hold = error_hold + error_lst\n",
    "        \n",
    "        if dq == True:\n",
    "            dqs_hold = dqs_hold + dq_lst\n",
    "            \n",
    "        if jit == True:\n",
    "            jitter_hold = jitter_hold + jitter_lst\n",
    "        \n",
    "    if jit == True:\n",
    "        # sort jitter vectors\n",
    "        jitter_dict = {}\n",
    "        for i in range(len(jitter_vector_list)):\n",
    "            jitter_dict[jitter_vector_list[i]] = [item[i] for item in jitter_hold]\n",
    "            # NORMALIZE JITTER VECTORS HERE\n",
    "        \n",
    "        \n",
    "    \n",
    "    if dq == True and jit == True:\n",
    "        return data_hold, headers_hold, jitter_dict, dqs_hold, error_hold\n",
    "    elif dq == True and jit == False:\n",
    "        return data_hold, headers_hold, dqs_hold, error_hold\n",
    "    elif dq == False and jit == True:\n",
    "        return data_hold, headers_hold, jitter_dict, error_hold\n",
    "    else:\n",
    "        return data_hold, headers_hold, error_hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the best medfilt window size of 5, find the points for which the residual value is \n",
    "# greater than 5 sigma and mark them - 1d\n",
    "def residual_outliers_1d(spectra_cut, n = 5): \n",
    "    medfilt_result = medfilt(spectra_cut, 5)\n",
    "    \n",
    "    residuals = medfilt_result-spectra_cut\n",
    "    stdev_residuals = 1.4826*np.nanmedian(np.abs(residuals - np.nanmedian(residuals)))\n",
    "    #stdev_residuals = np.sqrt(np.var(residuals))\n",
    "    outlier_locations = np.where(abs(residuals)>stdev_residuals*n)\n",
    "    \n",
    "    # option to plot positions of the outliers \n",
    "    #plt.figure(figsize=(15,10))\n",
    "    #plt.imshow(spectra_cut)\n",
    "    #plt.scatter(outlier_locations[1],outlier_locations[0], color = \"red\")\n",
    "    #plt.show()\n",
    "    \n",
    "    return outlier_locations\n",
    "\n",
    "# function to do a basic centroid trace fit to the data - should look into doing this with moffat instead\n",
    "def trace_spectrum(data, xi, xf, y_guess, profile_radius = 20, gauss_filter_width = 10):\n",
    "    # x-axis\n",
    "    x = np.arange(xi,xf)\n",
    "    # y-axis\n",
    "    y = np.arange(data.shape[0])\n",
    "    \n",
    "    # Define array that will save centroids at each x:\n",
    "    y_vals = np.zeros(len(x))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        # Find centroid within profile_radius pixels of the initial guess:\n",
    "        idx = np.where(np.abs(y-y_guess)<profile_radius)[0]\n",
    "        y_vals[i] = np.nansum(y[idx]*data[:,x[i]][idx])/np.nansum(data[:,x[i]][idx])\n",
    "        y_guess = y_vals[i]\n",
    "    return x,y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to mark bad pixels in files based on data quality (dq) frames included in the fits files\n",
    "# default flagged pixel to use is 16, but can add a list of whichever you want to remove\n",
    "def dq_clean(files, dqs, flags = [16]):\n",
    "    \n",
    "    # initializing list of bad pixels\n",
    "    bads = []\n",
    "    \n",
    "    # in each of the included files, mark the location of the pixels with the value given in the flags kwarg\n",
    "    for i in range(len(files)):\n",
    "        for j in flags:\n",
    "            bad = np.where(dqs[i] == j)\n",
    "            bad_indices = list(zip(bad[0], bad[1]))\n",
    "            bads.append(bad_indices)\n",
    "    bads = np.array(bads)\n",
    "    \n",
    "    # make a copy of the original fed in files array shape\n",
    "    files_clean = np.zeros_like(files)\n",
    "    \n",
    "    # make the value of each bad pixel -1, to be fit over later\n",
    "    for j in range(len(files)):\n",
    "        c_frame = files[j]\n",
    "        for index in bads[j]:\n",
    "            c_frame[index] = -1\n",
    "        files_clean[j] = c_frame\n",
    "    \n",
    "    # return the files with the bad pixel locations set as -1\n",
    "    return files_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_clean(files, difference_sigma, wind_size, sigma):\n",
    "    \n",
    "    # initializing lists\n",
    "    differences = []\n",
    "    labels = []\n",
    "    \n",
    "    # each image will be taken with a difference with the rest of the images in files\n",
    "    # i know it's redundant to take differences both ways, but i think it makes sense for the median files\n",
    "    for i in range(len(files)):\n",
    "        standard = files[i]\n",
    "        for j in range(len(files)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                diff = standard - files[j]\n",
    "                differences.append(diff)\n",
    "                label = [i,j]\n",
    "                labels.append(label)\n",
    "    \n",
    "    medians = []\n",
    "    \n",
    "    # create median frames using all of the subtractions from each frame together\n",
    "    for k in range(len(files)):\n",
    "        sublist = []\n",
    "        for n in range(len(labels)):\n",
    "            if labels[n][0] == k:\n",
    "                sublist.append(differences[n])\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        median_frame = np.nanmedian(sublist, axis = 0)   \n",
    "\n",
    "        medians.append(median_frame)\n",
    "        \n",
    "    # in each median frame, in each row sigma reject in windows given by wind_size and sigma\n",
    "    cr_loc = []\n",
    "    cr_loc_frame = []\n",
    "    files_clean = np.zeros_like(files)\n",
    "    frame_num = 0\n",
    "    for m in medians:\n",
    "        frame = np.copy(files[frame_num])\n",
    "        cr_loc_single = []\n",
    "        cr_loc_frame_single = np.zeros_like(medians[0])\n",
    "        for row_idx in range(len(m)):\n",
    "            row = m[row_idx]\n",
    "            row_med = np.nanmedian(row)\n",
    "            row_stdev = np.sqrt(np.var(row))\n",
    "            row_cut = row_med+(row_stdev*sigma)\n",
    "            q = wind_size\n",
    "            for p in range(len(row)-wind_size):\n",
    "                window = row[p:q]\n",
    "                wind_med = np.nanmedian(window)\n",
    "                wind_stdev = np.sqrt(np.var(m))\n",
    "                wind_cut = wind_med+(wind_stdev*sigma)\n",
    "                cut_use = max(wind_cut, row_cut)\n",
    "                for val in range(len(window)):\n",
    "                    if window[val] > cut_use:\n",
    "                        cr_loc_single.append([m,p+val])\n",
    "                        cr_loc_frame_single[row_idx][p+val] = 1\n",
    "                        frame[row_idx][p+val] = -2\n",
    "                p = p+wind_size\n",
    "                q = q+wind_size\n",
    "            \n",
    "        cr_loc.append(cr_loc_single)\n",
    "        cr_loc_frame.append(cr_loc_frame_single)\n",
    "        files_clean[frame_num] = frame \n",
    "        frame_num = frame_num + 1\n",
    "    \n",
    "    return files_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hc_clean(files, hc_sigma, hc_window_size):\n",
    "    \n",
    "    # initializing list\n",
    "    hcs = []\n",
    "    \n",
    "    splines_hcs = np.zeros_like(files)\n",
    "    files_clean = np.zeros_like(files)\n",
    "    \n",
    "    # hot and cold pixels\n",
    "    for frame_idx in range(len(files)):\n",
    "        frame = np.copy(files[frame_idx])\n",
    "        for column in range(len(files[0][0])):\n",
    "            test_spline = UnivariateSpline(np.arange(0,len(frame[:,column]),1), frame[:,column], s=100)\n",
    "            splines_hcs[frame_idx][:,column] = (test_spline(np.arange(0,len(frame[:,column]),1)))\n",
    "\n",
    "        # leave borders around the edges to not mess up the box, we don't really care about edges anyways\n",
    "        for p in range(hc_window_size+1,len(splines_hcs[frame_idx])-hc_window_size-1,1):\n",
    "            for q in range(hc_window_size+1,len(splines_hcs[frame_idx][0])-hc_window_size-1,1):\n",
    "                box = splines_hcs[frame_idx][p-hc_window_size:p+hc_window_size, q-hc_window_size:q+hc_window_size]\n",
    "                box_less = box[box != splines_hcs[frame_idx][p,q]]\n",
    "                box_med = np.nanmedian(box_less)\n",
    "                box_stdev = np.sqrt(np.var(box_less))\n",
    "                \n",
    "                if splines_hcs[frame_idx][p,q] < box_med - (box_stdev * hc_sigma) or splines_hcs[frame_idx][p,q] > box_med + (box_stdev * hc_sigma):\n",
    "                    hcs.append([p,q])\n",
    "                    frame[p][q] = -3\n",
    "        \n",
    "        files_clean[frame_idx] = frame\n",
    "    \n",
    "    return files_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spline_mark(files, traces, spline_sigma):\n",
    "\n",
    "    files_clean = np.zeros_like(files)\n",
    "    # take average of spline fits across frames\n",
    "    splines = np.zeros_like(files[0])\n",
    "    # for each column\n",
    "    for column in range(len(files[0][0])):\n",
    "        # create temporary list to hold the individual splines for that column\n",
    "        spline_fits = []\n",
    "        for i in files:\n",
    "            #test_spline = UnivariateSpline(np.arange(0,len(i[:,column]),1), i[:,column], s = 900)\n",
    "            test_spline = UnivariateSpline(np.arange(0,len(i[:,column]),1), i[:,column])\n",
    "            spline_fits.append(test_spline(np.arange(0,len(i[:,column]),1)))\n",
    "            #plt.plot(test_spline(np.arange(0,len(i[:,column]),1)))\n",
    "            #plt.show()\n",
    "\n",
    "        # take the median of the splines from each frame\n",
    "        med = np.nanmedian(spline_fits, axis = 0)\n",
    "        #plt.plot(med)\n",
    "        #plt.show()\n",
    "\n",
    "        # normalize the spline before appending\n",
    "        norm_med = med/np.nanmax(med)\n",
    "        splines[:,column] = norm_med\n",
    "    \n",
    "    # now, use the splines to go through each frame and reject problems\n",
    "    crs = []\n",
    "    spline_use = np.zeros_like(splines)\n",
    "    for frame_idx in range(len(files)):\n",
    "        frame = np.copy(files[frame_idx])\n",
    "        # use the median spline to reject cosmic rays for each frame's column\n",
    "        #for k in range(len(frame[0])):\n",
    "        # make this into groups? and then average sigma between them? no that won't work cause sigma changes too much\n",
    "        #for k in range(600,800,1):\n",
    "        for k in range(2,len(frame[0])-2,1):\n",
    "            spline_use_single = np.nanmedian(splines[:,k-2:k+2], axis = 1)\n",
    "            spline_use[:,k] = spline_use_single*np.nanmax(frame[:,k])\n",
    "            #plt.plot(spline_use[:,k])\n",
    "            #plt.show()\n",
    "            #plt.plot((splines[:,k]*np.nanmax(frame[:,k])))\n",
    "            #plt.plot((frame[:,k]))\n",
    "            #plt.yscale(\"log\")\n",
    "            #plt.show()\n",
    "            # scale the normalized median spline to the max of the frame before taking residual\n",
    "            resid = frame[:,k] - (spline_use[:,k])\n",
    "            resid_stdev = np.sqrt(np.var(resid))\n",
    "            cutoff = resid_stdev * spline_sigma\n",
    "            #print(cutoff)\n",
    "            #plt.plot(resid)\n",
    "            #plt.show()\n",
    "            #j = 0\n",
    "            for m in range(len(resid)):\n",
    "                # if the residual value is greater than residual stdev * sigma, then mark it as a problem\n",
    "                #print(traces[frame_idx][1][m])\n",
    "                if m > traces[frame_idx][1][m]+3 or m < traces[frame_idx][1][m]-3:\n",
    "                    #print(m)\n",
    "                    if resid[m] > cutoff:\n",
    "                        #cr_frame.append([k,m])\n",
    "                        #j = j+1\n",
    "                        frame[m,k] = -4\n",
    "                else:\n",
    "                    pass\n",
    "            #print(j)\n",
    "            # even after averaging, normalizing the spline and then scaling that to the max value of the new column, still strong \n",
    "            # residuals around the center at 10 sigma level\n",
    "                \n",
    "        files_clean[frame_idx] = frame\n",
    "        \n",
    "    return files_clean, splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spline_clean(files, splines = None):\n",
    "    if splines == None:\n",
    "        # take average of spline fits across frames\n",
    "        splines = np.zeros_like(files[0])\n",
    "        # for each column\n",
    "        for column in range(len(files[0][0])):\n",
    "            # create temporary list to hold the individual splines for that column\n",
    "            spline_fits = []\n",
    "            for i in files:\n",
    "                #test_spline = UnivariateSpline(np.arange(0,len(i[:,column]),1), i[:,column], s = 900)\n",
    "                test_spline = UnivariateSpline(np.arange(0,len(i[:,column]),1), i[:,column])\n",
    "                spline_fits.append(test_spline(np.arange(0,len(i[:,column]),1)))\n",
    "                #plt.plot(test_spline(np.arange(0,len(i[:,column]),1)))\n",
    "                #plt.show()\n",
    "\n",
    "            # take the median of the splines from each frame\n",
    "            med = np.nanmedian(spline_fits, axis = 0)\n",
    "            #plt.plot(med)\n",
    "            #plt.show()\n",
    "\n",
    "            # normalize the spline before appending\n",
    "            norm_med = med/np.nanmax(med)\n",
    "            splines[:,column] = norm_med\n",
    "        \n",
    "    # there are columns full of bad pixels - if an entire column has more than 50% pixels marked as bad (=-1)\n",
    "    # then use the average of the two surrounding columns \n",
    "    files_clean = np.zeros_like(files)\n",
    "    # use the splines to correct all bad pixels (<0)\n",
    "    for frame_idx in range(len(files)):\n",
    "        frame = np.copy(files[frame_idx])\n",
    "        #spline_use = np.zeros_like(splines)\n",
    "        #for column in range(len(frame[0])-1):\n",
    "        #    spline_use_c = splines[:,column]*np.nanmax(frame[:,column])\n",
    "        #    spline_use[:,column] = spline_use_c\n",
    "        for column in range(2,len(frame[0])-2,1):\n",
    "            bad_counter = 0\n",
    "            for pixel in range(len(frame)):\n",
    "                if frame[pixel][column] == -1:\n",
    "                    bad_counter = bad_counter + 1\n",
    "            #print(bad_counter)\n",
    "            if bad_counter >= len(frame)/3:\n",
    "                frame[:,column] = (frame[:,column-1]+frame[:,column+1])/2\n",
    "                #print(\"yes\")\n",
    "            for val in range(len(frame)):\n",
    "                #print(frame.shape, splines.shape)\n",
    "                if frame[val][column] in [-1,-2,-3,-4]:\n",
    "                    #print(\"yes\")\n",
    "                    #print(frame[val][column])\n",
    "                    #frame[val][column] = (spline_use[val][column])\n",
    "                    frame[val][column] = (frame[val][column+1]+frame[val][column-1])/2\n",
    "                    #print(frame[val][column])\n",
    "        files_clean[frame_idx] = frame\n",
    "        \n",
    "    return files_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how best to name the variables and still be able to feed them in? use the same name for all the arrays? seems dangerous\n",
    "def clean_data(files, dq_correct = True, dqs = None, difference_correct = True, difference_sigma = 5, \\\n",
    "               wind_size = 20, wind_sigma = 5, hc_correct = True, hc_sigma = 3, hc_wind_size = 2, \\\n",
    "               spline_correct = True, traces = None, spline_sigma = 3):\n",
    "    \n",
    "    if dq_correct == True:\n",
    "        if dqs == None:\n",
    "            print(\"Oops! You need the data quality frames corresponding to each exposure for the dq_correct.\" + \\\n",
    "                  \"You can get these from the get_data function with dq = True\")\n",
    "            return\n",
    "        else:\n",
    "            marked_1 = dq_clean(files, dqs)\n",
    "    else:\n",
    "        marked_1 = np.copy(files)\n",
    "        \n",
    "    if difference_correct == True:\n",
    "        marked_2 = difference_clean(marked_1, difference_sigma, wind_size, wind_sigma)\n",
    "    else: \n",
    "        marked_2 = np.copy(marked_1)\n",
    "    \n",
    "    if hc_correct == True:\n",
    "        marked_3 = hc_clean(marked_2, hc_sigma, hc_wind_size)\n",
    "    else:\n",
    "        marked_3 = np.copy(marked_2)\n",
    "        \n",
    "    if spline_correct == True:\n",
    "        if traces == None:\n",
    "            print(\"Oops! You need basic spectral traces for the spline fit option :)\")\n",
    "            return\n",
    "        else:\n",
    "            marked_4, splines = spline_mark(marked_3, traces, spline_sigma)\n",
    "            cleaned_data = spline_clean(marked_4, splines = splines)\n",
    "    else:\n",
    "        marked_4 = np.copy(marked_3)\n",
    "        cleaned_data = spline_clean(marked_4)\n",
    "    \n",
    "    return cleaned_data\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RA and Dec from header\n",
    "def times_to_bjd(headers):\n",
    "    times = []\n",
    "    exptimes = []\n",
    "    expstart = []\n",
    "    expend = []\n",
    "    for i in headers:\n",
    "        #times.append(i[\"DATE-OBS\"]+i[\"TIME-OBS\"])\n",
    "        times.append(i[\"DATE-OBS\"]+\"T\"+i[\"TIME-OBS\"])\n",
    "        #print(i[\"DATE-OBS\"])\n",
    "        exptimes.append(i[\"EXPTIME\"])\n",
    "        expstart.append(i[\"EXPSTART\"])\n",
    "        expend.append(i[\"EXPEND\"])\n",
    "\n",
    "    # change this time to bjd using package barycorr\n",
    "\n",
    "    jd_conv = 2400000.5\n",
    "    t_start = Time(np.array(expstart)+jd_conv, format='jd', scale='utc')\n",
    "    t_end = Time(np.array(expend)+jd_conv, format='jd', scale='utc')\n",
    "    #t = t.plot_date\n",
    "\n",
    "    t_start_bjd = utc_tdb.JDUTC_to_BJDTDB(t_start,starname = \"WASP-69\")#hip_id=8102 , lat=-30.169283, longi=-70.806789, alt=2241.9)\n",
    "    t_end_bjd = utc_tdb.JDUTC_to_BJDTDB(t_end,starname = \"WASP-69\")# , lat=-30.169283, longi=-70.806789, alt=2241.9)\n",
    "\n",
    "    return t_start_bjd, t_end_bjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_extraction(data, trace, method = \"optimal\", correct_bkg = False, aperture_radius = 15., ron = 1., gain = 1., \\\n",
    "                        nsigma = 10, polynomial_spacing = 0.75, polynomial_order = 3, errors = None):\n",
    "    \n",
    "    if method == \"optimal\":\n",
    "        spectrum = spectroscopy.getOptimalSpectrum(data, trace, aperture_radius, ron, gain, nsigma, polynomial_spacing, polynomial_order, data_variance = np.array(errors)**2)#, min_column = 600)\n",
    "    elif method == \"simple\":\n",
    "        x = np.arange(len(data)) # i think? test\n",
    "        spectrum = spectroscopy.getSimpleSpectrum(data, x, trace, aperture_radius, correct_bkg = correct_bkg, error_data = errors)#, min_column = 600)\n",
    "\n",
    "    return spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-correlation\n",
    "\n",
    "def xcorr(x,y):\n",
    "    \"\"\"\n",
    "    Perform Cross-Correlation on x and y Deviations\n",
    "    x    : 1st signal\n",
    "    y    : 2nd signal\n",
    "\n",
    "    returns\n",
    "    lags : lags of correlation\n",
    "    corr : coefficients of correlation\n",
    "    maxlag :  Lag at which cross-correlation function is maximized\n",
    "    \"\"\"\n",
    "    corr = signal.correlate(x-np.mean(x), y-np.mean(y), mode=\"full\")\n",
    "    scx=np.sum((y-np.mean(y))**2)\n",
    "    scy=np.sum((x-np.mean(x))**2)\n",
    "    corr = corr/(np.sqrt(scx*scy))\n",
    "    lags = signal.correlation_lags(len(x), len(y), mode=\"full\")\n",
    "    mindx=np.argmax(corr) #index of cross corr peak\n",
    "    \n",
    "    srad = 3 #Find peak shift from central region srad pixels wide on each side\n",
    "    sublag=lags[mindx-srad:mindx+(srad+1)]\n",
    "    subcf=corr[mindx-srad:mindx+(srad+1)]\n",
    "    result=np.polyfit(sublag, subcf, 2)\n",
    "    maxlag = - result[1]/(2.*result[0])\n",
    "    return lags, corr,maxlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD SYSTEMATICS OPTION FOR GP\n",
    "\n",
    "def model_light_curve(p, t, params, jitters):\n",
    "    \"\"\"\n",
    "    Generates a light curve with systematics. Uses the lmfit 'p' dictionary.\n",
    "    \"\"\"\n",
    "    params.t0  = p['t0']\n",
    "    params.per = p['per']\n",
    "    params.rp  = np.sqrt(p['rp2'])\n",
    "    params.a   = p['a']\n",
    "    params.inc = inclination(p['b'], p['a'])\n",
    "    params.ecc = p['ecc']\n",
    "    params.w   = p['w']\n",
    "    #params.limb_dark = \"quadratic\"\n",
    "    #params.u = [p['c1'], p['c2']]\n",
    "\n",
    "    if p['rp2'] == 0:\n",
    "        light_curve = np.ones(len(t)) # if it doesn't want a transit, give it a flat line\n",
    "    else:\n",
    "        light_curve = batman.TransitModel(params, t).light_curve(params)\n",
    "    \n",
    "    # zaf said best detrenders were the roll ones\n",
    "    systematics =  p[\"v2_roll\"]*np.array(jitters[\"V2_roll\"]) + p[\"v3_roll\"]*np.array(jitters[\"V3_roll\"]) + \\\n",
    "    p[\"lat\"]*np.array(jitters[\"Latitude\"]) + p[\"long\"]*np.array(jitters[\"Longitude\"]) + \\\n",
    "    p[\"RA\"]*np.array(jitters[\"RA\"]) + p[\"DEC\"]*np.array(jitters[\"DEC\"]) + 1 #p[\"offset\"]\n",
    "    \n",
    "    #systematics = p['xs']*xs + p['ys']*ys + p['xys']*xys + p['xs2']*xs2 + p['ys2']*ys2 \\\n",
    "    #           + p['common_mode']*common_mode + p['lin']*tlin + 1 \n",
    "    \n",
    "    #systematics = exp(-t/p['exp_tmscl']) + p['jitter_amp']*telescope_pos # leaving out systematics right now\n",
    "    \n",
    "    #plt.plot(light_curve)\n",
    "    #plt.plot(systematics)\n",
    "    \n",
    "    model = p['f0'] * light_curve * systematics\n",
    "    light_curve_plot = batman.TransitModel(params, t_final).light_curve(params)\n",
    "    plt.plot(t_final, p['f0'] * light_curve_plot)\n",
    "    #plt.plot(model)\n",
    "    #plt.plot(model)\n",
    "    #plt.show()\n",
    "    #print(model)\n",
    "    return model, systematics, light_curve\n",
    "\n",
    "    \n",
    "def residual(p, t, params, data, err, jitters):#, telescope_pos, err):\n",
    "    \"\"\"\n",
    "    Outputs the residual of the model and data.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model_light_curve(p, t, params, jitters)[0]#, telescope_pos)[0]\n",
    "    sys = model_light_curve(p, t, params, jitters)[1]\n",
    "    \n",
    "    plt.scatter(t, data/sys, color = \"purple\", label = \"with systematics\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(t, model)\n",
    "    plt.scatter(t, data, color = \"blue\", label = \"Original\", alpha = 0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    if err == None:\n",
    "        err = np.sqrt(p['f0']) # if no errorbars specified, assume shot noise uncertainty from baseline flux\n",
    "\n",
    "    chi2 = sum((data-model)**2/err**2)\n",
    "    print(chi2)\n",
    "    res = np.std((data-model)/max(model))\n",
    "    \n",
    "    return (data-model)/err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limb Darkeneing\n",
    "@custom_model\n",
    "def nonlinear_limb_darkening(x, c0=0.0, c1=0.0, c2=0.0, c3=0.0):\n",
    "    \"\"\"\n",
    "    Define non-linear limb darkening model with four parameters c0, c1, c2, c3.\n",
    "    \"\"\"\n",
    "    model = (1. - (c0 * (1. - x ** (1. / 2)) + c1 * (1. - x ** (2. / 2)) + c2 * (1. - x ** (3. / 2)) + c3 *\n",
    "                   (1. - x ** (4. / 2))))\n",
    "    return model\n",
    "\n",
    "\n",
    "@custom_model\n",
    "def quadratic_limb_darkening(x, aLD=0.0, bLD=0.0):\n",
    "    \"\"\"\n",
    "    Define linear limb darkening model with parameters aLD and bLD.\n",
    "    \"\"\"\n",
    "    model = 1. - aLD * (1. - x) - bLD * (1. - x) ** (4. / 2.)\n",
    "    return model\n",
    "\n",
    "def limb_dark_fit(grating, wsdata, M_H, Teff, logg, dirsen, ld_model='1D'):\n",
    "    \"\"\"\n",
    "    Calculates stellar limb-darkening coefficients for a given wavelength bin.\n",
    "\n",
    "    Currently supports:\n",
    "    HST STIS G750L, G750M, G430L gratings\n",
    "    HST WFC3 UVIS/G280, IR/G102, IR/G141 grisms\n",
    "\n",
    "    What is used for 1D models - Kurucz (?)\n",
    "    Procedure from Sing et al. (2010, A&A, 510, A21).\n",
    "    Uses 3D limb darkening from Magic et al. (2015, A&A, 573, 90).\n",
    "    Uses photon FLUX Sum over (lambda*dlamba).\n",
    "    :param grating: string; grating to use ('G430L','G750L','G750M', 'G280', 'G102', 'G141')\n",
    "    :param wsdata: array; data wavelength solution\n",
    "    :param M_H: float; stellar metallicity\n",
    "    :param Teff: float; stellar effective temperature (K)\n",
    "    :param logg: float; stellar gravity\n",
    "    :param dirsen: string; path to main limb darkening directory\n",
    "    :param ld_model: string; '1D' or '3D', makes choice between limb darkening models; default is 1D\n",
    "    :return: uLD: float; linear limb darkening coefficient\n",
    "    aLD, bLD: float; quadratic limb darkening coefficients\n",
    "    cp1, cp2, cp3, cp4: float; three-parameter limb darkening coefficients\n",
    "    c1, c2, c3, c4: float; non-linear limb-darkening coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    print('You are using the', str(ld_model), 'limb darkening models.')\n",
    "\n",
    "    if ld_model == '1D':\n",
    "\n",
    "        direc = os.path.join(dirsen, 'Kurucz')\n",
    "\n",
    "        print('Current Directories Entered:')\n",
    "        print('  ' + dirsen)\n",
    "        print('  ' + direc)\n",
    "\n",
    "        # Select metallicity\n",
    "        M_H_Grid = np.array([-0.1, -0.2, -0.3, -0.5, -1.0, -1.5, -2.0, -2.5, -3.0, -3.5, -4.0, -4.5, -5.0, 0.0, 0.1, 0.2, 0.3, 0.5, 1.0])\n",
    "        M_H_Grid_load = np.array([0, 1, 2, 3, 5, 7, 8, 9, 10, 11, 12, 13, 14, 17, 20, 21, 22, 23, 24])\n",
    "        optM = (abs(M_H - M_H_Grid)).argmin()\n",
    "        MH_ind = M_H_Grid_load[optM]\n",
    "\n",
    "        # Determine which model is to be used, by using the input metallicity M_H to figure out the file name we need\n",
    "        direc = 'Kurucz'\n",
    "        file_list = 'kuruczlist.sav'\n",
    "        sav1 = readsav(os.path.join(dirsen, file_list))\n",
    "        model = bytes.decode(sav1['li'][MH_ind])  # Convert object of type \"byte\" to \"string\"\n",
    "\n",
    "        # Select Teff and subsequently logg\n",
    "        Teff_Grid = np.array([3500, 3750, 4000, 4250, 4500, 4750, 5000, 5250, 5500, 5750, 6000, 6250, 6500])\n",
    "        optT = (abs(Teff - Teff_Grid)).argmin()\n",
    "\n",
    "        logg_Grid = np.array([4.0, 4.5, 5.0])\n",
    "        optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        if logg_Grid[optG] == 4.0:\n",
    "            Teff_Grid_load = np.array([8, 19, 30, 41, 52, 63, 74, 85, 96, 107, 118, 129, 138])\n",
    "\n",
    "        elif logg_Grid[optG] == 4.5:\n",
    "            Teff_Grid_load = np.array([9, 20, 31, 42, 53, 64, 75, 86, 97, 108, 119, 129, 139])\n",
    "\n",
    "        elif logg_Grid[optG] == 5.0:\n",
    "            Teff_Grid_load = np.array([10, 21, 32, 43, 54, 65, 76, 87, 98, 109, 120, 130, 140])\n",
    "\n",
    "        # Where in the model file is the section for the Teff we want? Index T_ind tells us that.\n",
    "        T_ind = Teff_Grid_load[optT]\n",
    "        header_rows = 3    #  How many rows in each section we ignore for the data reading\n",
    "        data_rows = 1221   # How  many rows of data we read\n",
    "        line_skip_data = (T_ind + 1) * header_rows + T_ind * data_rows   # Calculate how many lines in the model file we need to skip in order to get to the part we need (for the Teff we want).\n",
    "        line_skip_header = T_ind * (data_rows + header_rows)\n",
    "\n",
    "        # Read the header, in case we want to have the actual Teff, logg and M_H info.\n",
    "        # headerinfo is a pandas object.\n",
    "        headerinfo = pd.read_csv(os.path.join(dirsen, direc, model), delim_whitespace=True, header=None,\n",
    "                                 skiprows=line_skip_header, nrows=1)\n",
    "\n",
    "        Teff_model = headerinfo[1].values[0]\n",
    "        logg_model = headerinfo[3].values[0]\n",
    "        MH_model = headerinfo[6].values[0]\n",
    "        MH_model = float(MH_model[1:-1])\n",
    "\n",
    "        print('\\nClosest values to your inputs:')\n",
    "        print('Teff: ', Teff_model)\n",
    "        print('M_H: ', MH_model)\n",
    "        print('log(g): ', logg_model)\n",
    "\n",
    "        # Read the data; data is a pandas object.\n",
    "        data = pd.read_csv(os.path.join(dirsen, direc, model), delim_whitespace=True, header=None,\n",
    "                              skiprows=line_skip_data, nrows=data_rows)\n",
    "\n",
    "        # Unpack the data\n",
    "        ws = data[0].values * 10   # Import wavelength data\n",
    "        f0 = data[1].values / (ws * ws)\n",
    "        f1 = data[2].values * f0 / 100000.\n",
    "        f2 = data[3].values * f0 / 100000.\n",
    "        f3 = data[4].values * f0 / 100000.\n",
    "        f4 = data[5].values * f0 / 100000.\n",
    "        f5 = data[6].values * f0 / 100000.\n",
    "        f6 = data[7].values * f0 / 100000.\n",
    "        f7 = data[8].values * f0 / 100000.\n",
    "        f8 = data[9].values * f0 / 100000.\n",
    "        f9 = data[10].values * f0 / 100000.\n",
    "        f10 = data[11].values * f0 / 100000.\n",
    "        f11 = data[12].values * f0 / 100000.\n",
    "        f12 = data[13].values * f0 / 100000.\n",
    "        f13 = data[14].values * f0 / 100000.\n",
    "        f14 = data[15].values * f0 / 100000.\n",
    "        f15 = data[16].values * f0 / 100000.\n",
    "        f16 = data[17].values * f0 / 100000.\n",
    "\n",
    "        # Make single big array of them\n",
    "        fcalc = np.array([f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16])\n",
    "        phot1 = np.zeros(fcalc.shape[0])\n",
    "\n",
    "        # Define mu\n",
    "        mu = np.array([1.000, .900, .800, .700, .600, .500, .400, .300, .250, .200, .150, .125, .100, .075, .050, .025, .010])\n",
    "\n",
    "        # Passed on to main body of function are: ws, fcalc, phot1, mu\n",
    "\n",
    "    elif ld_model == '3D':\n",
    "\n",
    "        direc = os.path.join(dirsen, '3DGrid')\n",
    "\n",
    "        print('Current Directories Entered:')\n",
    "        print('  ' + dirsen)\n",
    "        print('  ' + direc)\n",
    "\n",
    "        # Select metallicity\n",
    "        M_H_Grid = np.array([-3.0, -2.0, -1.0, 0.0])  # Available metallicity values in 3D models\n",
    "        M_H_Grid_load = ['30', '20', '10', '00']  # The according identifiers to individual available M_H values\n",
    "        optM = (abs(M_H - M_H_Grid)).argmin()  # Find index at which the closes M_H values from available values is to the input M_H.\n",
    "\n",
    "        # Select Teff\n",
    "        Teff_Grid = np.array([4000, 4500, 5000, 5500, 5777, 6000, 6500, 7000])  # Available Teff values in 3D models\n",
    "        optT = (abs(Teff - Teff_Grid)).argmin()  # Find index at which the Teff values is, that is closest to input Teff.\n",
    "\n",
    "        # Select logg, depending on Teff. If several logg possibilities are given for one Teff, pick the one that is\n",
    "        # closest to user input (logg).\n",
    "\n",
    "        if Teff_Grid[optT] == 4000:\n",
    "            logg_Grid = np.array([1.5, 2.0, 2.5])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 4500:\n",
    "            logg_Grid = np.array([2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 5000:\n",
    "            logg_Grid = np.array([2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 5500:\n",
    "            logg_Grid = np.array([3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 5777:\n",
    "            logg_Grid = np.array([4.4])\n",
    "            optG = 0\n",
    "\n",
    "        elif Teff_Grid[optT] == 6000:\n",
    "            logg_Grid = np.array([3.5, 4.0, 4.5])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 6500:\n",
    "            logg_Grid = np.array([4.0, 4.5])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 7000:\n",
    "            logg_Grid = np.array([4.5])\n",
    "            optG = 0\n",
    "\n",
    "        # Select Teff and Log g. Mtxt, Ttxt and Gtxt are then put together as string to load correct files.\n",
    "        Mtxt = M_H_Grid_load[optM]\n",
    "        Ttxt = \"{:2.0f}\".format(Teff_Grid[optT] / 100)\n",
    "        if Teff_Grid[optT] == 5777:\n",
    "            Ttxt = \"{:4.0f}\".format(Teff_Grid[optT])\n",
    "        Gtxt = \"{:2.0f}\".format(logg_Grid[optG] * 10)\n",
    "\n",
    "        #\n",
    "        file = 'mmu_t' + Ttxt + 'g' + Gtxt + 'm' + Mtxt + 'v05.flx'\n",
    "        print('Filename:', file)\n",
    "\n",
    "        # Read data from IDL .sav file\n",
    "        sav = readsav(os.path.join(direc, file))  # readsav reads an IDL .sav file\n",
    "        ws = sav['mmd'].lam[0]  # read in wavelength\n",
    "        flux = sav['mmd'].flx  # read in flux\n",
    "        Teff_model = Teff_Grid[optT]\n",
    "        logg_model = logg_Grid[optG]\n",
    "        MH_model = str(M_H_Grid[optM])\n",
    "\n",
    "        print('\\nClosest values to your inputs:')\n",
    "        print('Teff  : ', Teff_model)\n",
    "        print('M_H   : ', MH_model)\n",
    "        print('log(g): ', logg_model)\n",
    "\n",
    "        f0 = flux[0]\n",
    "        f1 = flux[1]\n",
    "        f2 = flux[2]\n",
    "        f3 = flux[3]\n",
    "        f4 = flux[4]\n",
    "        f5 = flux[5]\n",
    "        f6 = flux[6]\n",
    "        f7 = flux[7]\n",
    "        f8 = flux[8]\n",
    "        f9 = flux[9]\n",
    "        f10 = flux[10]\n",
    "\n",
    "        # Make single big array of them\n",
    "        fcalc = np.array([f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10])\n",
    "        phot1 = np.zeros(fcalc.shape[0])\n",
    "\n",
    "        # Mu from grid\n",
    "        # 0.00000    0.0100000    0.0500000     0.100000     0.200000     0.300000   0.500000     0.700000     0.800000     0.900000      1.00000\n",
    "        mu = sav['mmd'].mu\n",
    "\n",
    "        # Passed on to main body of function are: ws, fcalc, phot1, mu\n",
    "\n",
    "    ### Load response function and interpolate onto kurucz model grid\n",
    "\n",
    "    # FOR STIS\n",
    "    if grating == 'G430L':\n",
    "        sav = readsav(os.path.join(dirsen, 'G430L.STIS.sensitivity.sav'))  # wssens,sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 3\n",
    "\n",
    "    if grating == 'G750M':\n",
    "        sav = readsav(os.path.join(dirsen, 'G750M.STIS.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 0.554\n",
    "\n",
    "    if grating == 'G750L':\n",
    "        sav = readsav(os.path.join(dirsen, 'G750L.STIS.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 4.882\n",
    "\n",
    "    # FOR WFC3\n",
    "    if grating == 'G141':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'G141.WFC3.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 1\n",
    "\n",
    "    if grating == 'G102':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'G141.WFC3.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 1\n",
    "\n",
    "    if grating == 'G280':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'G280.WFC3.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 1\n",
    "\n",
    "    # FOR JWST\n",
    "    if grating == 'NIRSpecPrism':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'NIRSpec.prism.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 12\n",
    "\n",
    "\n",
    "    widek = np.arange(len(wsdata))\n",
    "    wsHST = wssens\n",
    "    wsHST = np.concatenate((np.array([wsHST[0] - wdel - wdel, wsHST[0] - wdel]),\n",
    "                            wsHST,\n",
    "                            np.array([wsHST[len(wsHST) - 1] + wdel,\n",
    "                                      wsHST[len(wsHST) - 1] + wdel + wdel])))\n",
    "\n",
    "    respoutHST = sensitivity / np.max(sensitivity)\n",
    "    respoutHST = np.concatenate((np.zeros(2), respoutHST, np.zeros(2)))\n",
    "    inter_resp = interp1d(wsHST, respoutHST, bounds_error=False, fill_value=0)\n",
    "    respout = inter_resp(ws)  # interpolate sensitivity curve onto model wavelength grid\n",
    "\n",
    "    wsdata = np.concatenate((np.array([wsdata[0] - wdel - wdel, wsdata[0] - wdel]), wsdata,\n",
    "                             np.array([wsdata[len(wsdata) - 1] + wdel, wsdata[len(wsdata) - 1] + wdel + wdel])))\n",
    "    respwavebin = wsdata / wsdata * 0.0\n",
    "    widek = widek + 2  # need to add two indicies to compensate for padding with 2 zeros\n",
    "    respwavebin[widek] = 1.0\n",
    "    data_resp = interp1d(wsdata, respwavebin, bounds_error=False, fill_value=0)\n",
    "    reswavebinout = data_resp(ws)  # interpolate data onto model wavelength grid\n",
    "\n",
    "    # Integrate over the spectra to make synthetic photometric points.\n",
    "    for i in range(fcalc.shape[0]):  # Loop over spectra at diff angles\n",
    "        fcal = fcalc[i, :]\n",
    "        Tot = int_tabulated(ws, ws * respout * reswavebinout)\n",
    "        phot1[i] = (int_tabulated(ws, ws * respout * reswavebinout * fcal, sort=True)) / Tot\n",
    "\n",
    "    if ld_model == '1D':\n",
    "        yall = phot1 / phot1[0]\n",
    "    elif ld_model == '3D':\n",
    "        yall = phot1 / phot1[10]\n",
    "\n",
    "    Co = np.zeros((6, 4))   # NOT-REUSED\n",
    "\n",
    "    A = [0.0, 0.0, 0.0, 0.0]  # c1, c2, c3, c4      # NOT-REUSED\n",
    "    x = mu[1:]     # wavelength\n",
    "    y = yall[1:]   # flux\n",
    "    weights = x / x   # NOT-REUSED\n",
    "\n",
    "    # Start fitting the different models\n",
    "    fitter = LevMarLSQFitter()\n",
    "\n",
    "    # Fit a four parameter non-linear limb darkening model and get fitted variables, c1, c2, c3, c4.\n",
    "    corot_4_param = nonlinear_limb_darkening()\n",
    "    corot_4_param = fitter(corot_4_param, x, y)\n",
    "    c1, c2, c3, c4 = corot_4_param.parameters\n",
    "\n",
    "    # Fit a three parameter non-linear limb darkening model and get fitted variables, cp2, cp3, cp4 (cp1 = 0).\n",
    "    corot_3_param = nonlinear_limb_darkening()\n",
    "    corot_3_param.c0.fixed = True  # 3 param is just 4 param with c0 = 0.0\n",
    "    corot_3_param = fitter(corot_3_param, x, y)\n",
    "    cp1, cp2, cp3, cp4 = corot_3_param.parameters\n",
    "\n",
    "    # Fit a quadratic limb darkening model and get fitted parameters aLD and bLD.\n",
    "    quadratic = quadratic_limb_darkening()\n",
    "    quadratic = fitter(quadratic, x, y)\n",
    "    aLD, bLD = quadratic.parameters\n",
    "\n",
    "    # Fit a linear limb darkening model and get fitted variable uLD.\n",
    "    linear = nonlinear_limb_darkening()\n",
    "    linear.c0.fixed = True\n",
    "    linear.c2.fixed = True\n",
    "    linear.c3.fixed = True\n",
    "    linear = fitter(linear, x, y)\n",
    "    uLD = linear.c1.value\n",
    "\n",
    "    print('\\nLimb darkening parameters:')\n",
    "    print(\"4param \\t{:0.8f}\\t{:0.8f}\\t{:0.8f}\\t{:0.8f}\".format(c1, c2, c3, c4))\n",
    "    print(\"3param \\t{:0.8f}\\t{:0.8f}\\t{:0.8f}\".format(cp2, cp3, cp4))\n",
    "    print(\"Quad \\t{:0.8f}\\t{:0.8f}\".format(aLD, bLD))\n",
    "    print(\"Linear \\t{:0.8f}\".format(uLD))\n",
    "\n",
    "    return uLD, c1, c2, c3, c4, cp1, cp2, cp3, cp4, aLD, bLD\n",
    "\n",
    "\n",
    "def int_tabulated(X, F, sort=False):\n",
    "    Xsegments = len(X) - 1\n",
    "\n",
    "    # Sort vectors into ascending order.\n",
    "    if not sort:\n",
    "        ii = np.argsort(X)\n",
    "        X = X[ii]\n",
    "        F = F[ii]\n",
    "\n",
    "    while (Xsegments % 4) != 0:\n",
    "        Xsegments = Xsegments + 1\n",
    "\n",
    "    Xmin = np.min(X)\n",
    "    Xmax = np.max(X)\n",
    "\n",
    "    # Uniform step size.\n",
    "    h = (Xmax + 0.0 - Xmin) / Xsegments\n",
    "    # Compute the interpolates at Xgrid.\n",
    "    # x values of interpolates >> Xgrid = h * FINDGEN(Xsegments + 1L) + Xmin\n",
    "    z = splev(h * np.arange(Xsegments + 1) + Xmin, splrep(X, F))\n",
    "\n",
    "    # Compute the integral using the 5-point Newton-Cotes formula.\n",
    "    ii = (np.arange((len(z) - 1) / 4, dtype=int) + 1) * 4\n",
    "\n",
    "    return np.sum(2.0 * h * (7.0 * (z[ii - 4] + z[ii]) + 32.0 * (z[ii - 3] + z[ii - 1]) + 12.0 * z[ii - 2]) / 45.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
